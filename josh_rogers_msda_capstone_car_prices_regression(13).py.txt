# -*- coding: utf-8 -*-
"""Josh_Rogers_MSDA_Capstone_Car_Prices_Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cdsc_l9t2PkBoiyMqpufy68g_GXuvFX6
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats
from scipy.stats import shapiro
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import Lasso
import statsmodels.formula.api as ols
from sklearn.metrics import mean_squared_error
from scipy.stats import zscore
import re
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

df = pd.read_csv('Car_Prices_Final.csv',usecols=['year','make','body','transmission','odometer','color','Price_Diff', 'condition'])

print(df.head())
print(df.shape)
print(df.info())
print(df.describe())

categorical_vars = ['make', 'body', 'transmission', 'color', 'condition']
continuous_vars = ['year', 'odometer', 'Price_Diff']

# Create a new object that omits null rows
df_filtered = df.dropna()

summary_stats = df_filtered.describe()[['year', 'odometer', 'Price_Diff']]
print(summary_stats)
print(df_filtered.shape)

# Calculate Q1 (25th percentile) and Q3 (75th percentile) for each numerical column
Q1 = df_filtered[['year', 'odometer', 'Price_Diff']].quantile(0.25)
Q3 = df_filtered[['year', 'odometer', 'Price_Diff']].quantile(0.75)

# Calculate the IQR for each numerical column
IQR = Q3 - Q1

# Define the lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers for each numerical column
outliers = ((df_filtered[['year', 'odometer', 'Price_Diff']] < lower_bound) | (df_filtered[['year', 'odometer', 'Price_Diff']] > upper_bound))

# Count the number of outliers for each column
outlier_counts = outliers.sum()
print("Outlier Counts:")
print(outlier_counts)

df_filtered = df_filtered[~outliers.any(axis=1)]

# Plotting distribution of key variables before outlier removal
plt.figure(figsize=(12, 6))

# Plot histograms for 'year', 'odometer', and 'Price_Diff' before removing outliers
for i, col in enumerate(continuous_vars, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df[col], kde=True, bins=30, color='blue')
    plt.title(f'Distribution of {col} (Before Outlier Removal)')

plt.tight_layout()
plt.show()

# Plotting distribution of key variables after outlier removal
plt.figure(figsize=(12, 6))

# Plot histograms for 'year', 'odometer', and 'Price_Diff' after removing outliers
for i, col in enumerate(continuous_vars, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df_filtered[col], kde=True, bins=30, color='green')
    plt.title(f'Distribution of {col} (After Outlier Removal)')

plt.tight_layout()
plt.show()

df_filtered = df_filtered.copy()

# Fill missing values
df_filtered.loc[:, 'condition'] = df_filtered['condition'].fillna('Unknown')

# Create 'condition_category'
df_filtered.loc[:, 'condition_category'] = np.where(
    df_filtered['condition'] == 'Unknown', 'Unknown',
    np.where(df_filtered['condition'] <= 15, 'Poor',
             np.where(df_filtered['condition'] <= 30, 'Fair',
                      np.where(df_filtered['condition'] <= 40, 'Good', 'Excellent'))))

print(df_filtered[['condition', 'condition_category']].head())

label_encoder = LabelEncoder()
df_filtered['condition_encoded'] = label_encoder.fit_transform(df_filtered['condition_category'])

categorical_vars = ['make', 'body', 'transmission', 'condition_encoded']

# Apply one-hot encoding to categorical variables (make, color, etc.)
df_ohe = pd.get_dummies(df_filtered[categorical_vars], drop_first=True, dtype=int)

# Concatenate numerical columns
df_ohe = pd.concat([df_ohe, df_filtered[['year', 'odometer', 'Price_Diff']]], axis=1)

# Check the resulting dataframe
print(df_ohe.head())
print(df_ohe.columns.tolist())

# Add constant to the dataset for the intercept
X = add_constant(df_ohe)

# Calculate VIF for each variable
vif_data = pd.DataFrame()
vif_data['Variable'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Print the VIF data
print("VIF Results:")
print(vif_data)

# Remove variables with high VIF
high_vif = vif_data[vif_data['VIF'] > 10]
print("Variables with high VIF (> 10):")
print(high_vif)

# Drop variables with high VIF
df_ohe.drop(columns=[
    'make_Chevrolet', 'make_Ford', 'body_Convertible', 'body_Coupe',
    'body_Crew Cab', 'body_Extended Cab', 'body_G Sedan', 'body_Hatchback',
    'body_Minivan', 'body_Quad Cab', 'body_Regular Cab', 'body_SUV',
    'body_Sedan', 'body_SuperCab', 'body_SuperCrew', 'body_Van', 'body_Wagon'
], inplace=True)

print(df_ohe.head())
print(df_ohe.columns.tolist())

df_ohe.columns = df_ohe.columns.str.replace(' ', '_').str.replace('-', '_')

valid_columns = df_ohe.columns.difference(['Price_Diff'])  # Exclude dependent variable
formula = 'Price_Diff ~ ' + ' + '.join(valid_columns)

from statsmodels.formula.api import ols
df_model = ols(formula, data=df_ohe).fit()

print(df_model.summary())

p_values = df_model.pvalues

# Filter the variables with p-values less than 0.05
significant_vars = p_values[p_values < 0.05].index

# Exclude the intercept from the variables
significant_vars = significant_vars.difference(['Intercept'])
formula_significant = 'Price_Diff ~ ' + ' + '.join(significant_vars)

# Fit the new model
df_model_significant = ols(formula_significant, data=df_ohe).fit()

print(df_model_significant.summary())

X = df_ohe.drop('Price_Diff', axis = 1)
y = df_ohe['Price_Diff']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Lasso regression
lasso = Lasso(alpha=1.0, max_iter=5000)
lasso.fit(X_train_scaled, y_train)

selected_features = X.columns[lasso.coef_ != 0]
print(f"Selected features: {selected_features}")

# Get the coefficients
coef = lasso.coef_

# Create a DataFrame
coef_df = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': coef
})

# Sort by the abs
coef_df['abs_coef'] = coef_df['Coefficient'].abs()
coef_df = coef_df.sort_values(by='abs_coef', ascending=False)

# Select the top 10 features
top_features = coef_df.head(10)

# Print the top 10 features and their coefficients
print("Top 10 LASSO Regression Coefficients:")
print(top_features[['Feature', 'Coefficient']])
print("")

plt.figure(figsize=(12, 6))
top_features = coef_df[coef_df['abs_coef'] > 0].head(10)
sns.barplot(data=top_features, x='Coefficient', y='Feature', hue='Feature', dodge=False, palette='coolwarm', legend=False)
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.title('Top 10 Lasso Regression Coefficients')

# Show plot
plt.show()

y_pred = lasso.predict(X_test_scaled)
residuals = y_test - y_pred

shapiro_stat, shapiro_p = shapiro(residuals)

# Print results
print(f'Shapiro-Wilk Test Statistic: {shapiro_stat}')
print(f'P-value: {shapiro_p}')

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse}")

# Calculate Residuals
residuals = y_test - y_pred

# Residual Standard Error (RSE)
rse = np.sqrt(np.sum(residuals**2) / (len(y_test) - X_test.shape[1] - 1))
print(f"Residual Standard Error (RSE): {rse}")

plt.figure(figsize = (10,6))
plt.scatter(y_pred, residuals, color = 'blue', alpha = 0.5)
plt.axhline(y = 0, color = 'red', linestyle = '--')
plt.title("Residuals vs Fitted Values")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.show()

plt.figure(figsize = (10,6))
sns.histplot(residuals, kde = True, color = 'blue')
plt.title("Histogram of Residuals")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.show()

plt.figure(figsize = (10,6))
stats.probplot(residuals, dist = "norm", plot = plt)
plt.title("Q-Q Plot of Residuals")
plt.show()

# Predicted vs Actual Plot
plt.figure(figsize = (8, 6))
plt.scatter(y_pred, y_test, alpha = 0.5, color = 'blue', label = 'Predicted vs Actual')
plt.plot([min(y_pred), max(y_pred)], [min(y_pred), max(y_pred)], color = 'red', linestyle = '--', lw = 2, label = 'Ideal Line')
plt.title('Predicted vs Actual (Price_Diff)')
plt.xlabel('Predicted Values')
plt.ylabel('Actual Values')
plt.legend()
plt.show()

# Residuals vs Fitted Plot
plt.figure(figsize = (8, 6))
sns.residplot(x = y_pred, y = y_test - y_pred, lowess = True, line_kws={'color': 'red'}, scatter_kws = {'alpha': 0.5})
plt.title('Residuals vs Fitted Values')
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.legend(['Residuals', 'Lowess Line'], loc='upper right')
plt.show()

# Univariate Analysis
plt.figure(figsize=(12, 6))
for i, col in enumerate(continuous_vars, 1):
    plt.subplot(2, 2, i)
    sns.histplot(df_filtered[col], kde=True, bins=30, color='blue')
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Function to plot a horizontal barplot for large categorical variables
def plot_large_categorical_distribution(df, column, top_n=15):
    # Count the occurrences of each category in the column
    category_counts = df[column].value_counts().head(top_n)

    # Create a horizontal bar plot for the top_n categories
    plt.figure(figsize=(10, 6))
    sns.barplot(x=category_counts.values, y=category_counts.index, palette='Set2')

    # Customize the plot
    plt.title(f'Top {top_n} {column.capitalize()} Categories', fontsize=16)
    plt.xlabel('Count', fontsize=12)
    plt.ylabel(column.capitalize(), fontsize=12)
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)
    plt.tight_layout()
    plt.show()

# Plot for 'make' (top 15)
plot_large_categorical_distribution(df_filtered, 'make', top_n=15)

# Plot for 'body' (top 15)
plot_large_categorical_distribution(df_filtered, 'body', top_n=15)

# Histogram for 'condition' with 5 bins
plt.figure(figsize=(10, 6))
sns.histplot(df_filtered['condition_encoded'], bins=5, kde=False, color='blue')
plt.title('Histogram of Condition with 5 Bins', fontsize=16)
plt.xlabel('Condition (Encoded)', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.tight_layout()
plt.show()



# Correlation Heatmap
plt.figure(figsize=(10, 6))
corr_matrix = df_filtered[continuous_vars].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of Continuous Variables')
plt.show()

# Export the cleaned and processed data to a CSV file for Tableau
file_path = "price_diff_analysis.csv"
df_ohe.to_csv(file_path, index=False)

print(f"File saved as {file_path}")